import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions.{col, from_json}
import org.apache.spark.sql.types.{IntegerType, StringType, StructType}

object  lectorjson {
  def main(args:Array[String]):Unit={

    /*Establecemos la conexi√≥n utilizando el SparkSession, dando nombre a la app
    y utilizando 2 local para que se produzca el streaming.*/
    val spark=SparkSession.builder().appName("lectorjson").master("local[2]").getOrCreate()

    //Lectura en streaming con formato kafka.
    val df=spark.readStream
      .format("kafka")
      .option("kafka.bootstrap.servers", "localhost:9092")
      .option("subscribe","topicprueba3")
      .option("startingOffSets","earliest")
      .load()

    /*El formato kafka no es legible, por lo que debemos hacer un casteo
    de todos los datos para convertirlo en algo legible, en este caso en String.*/
    val res=df.selectExpr("CAST(value AS STRING)")

    //Creamos la estructura que queremos que tenga ese esquema
    val schema= new StructType().add("id",IntegerType)
      .add("first_name",StringType)
      .add("last_name",StringType)
      .add("email",StringType)
      .add("gender",StringType)
      .add("ip_address",StringType)

    /*Hacer que el res y el esquema sean legibles. Vamos a recibir
    un casteo de algo que tiene formato json, aplicale el esuqema y dale un alias.*/
    val personal=res.select(from_json(col("value"),schema).as("data"))
      .select("data.*")//Consulta para que nos muestre los datos
      print("Mostrar datos por consola")

    //Escribir la consulta que acabamos de realizar por consola
    val query=personal.writeStream
      .format("console")
      .outputMode("append")
      .start()
      .awaitTermination()

  }
}